# web_page_crawling

- 웹페이지의 텍스트 데이터를 수집하기 위한 서비스입니다.

<br>

## requirements
- selenium : _pip install selenium_
- pandas : _pip install pandas_
  

- chrome webdriver : https://sites.google.com/a/chromium.org/chromedriver/downloads

1. chrome 웹 브라우저를 실행하고 chrome://version을 주소창에 입력해 버전을 확인한 뒤 자신에게 맞는 버전을 다운받습니다.

2. 다운 받은 chromedriver.exe를 blog_crawling.py파일과 동일한 경로에 삽입합니다. 
<br>

## 사용법
blog_crawling.py 파일 open

<img width="408" alt="test" src="https://user-images.githubusercontent.com/59256704/125067886-c9a87c80-e0ef-11eb-9ee3-27e3fb98ed8e.png">
세번째 파라미터값을 다운받은 chromedriver.exe의 경로를 삽입하고 실행